# mi-pipeline
Pipeline ETL automatizado para procesamiento de datos del clima actual

## ğŸ“‹ DescripciÃ³n

Weather ETL Pipeline es un pipeline end-to-end desarrollado con Apache Airflow, ejecutado localmente mediante Docker Compose, que automatiza la extracciÃ³n, transformaciÃ³n y carga de datos del clima.
El flujo consulta una fuente de datos climÃ¡ticos (simulada o API real), calcula mÃ©tricas derivadas (como temperatura en Fahrenheit e Ã­ndice de confort), y finalmente almacena los resultados en un archivo JSON para anÃ¡lisis posterior.

Este proyecto fue desarrollado como parte del taller "Mi Primer Pipeline con Airflow", integrando buenas prÃ¡cticas de orquestaciÃ³n y diseÃ±o ETL.

## ğŸ¯ Objetivos

- Extraer informaciÃ³n meteorolÃ³gica de una fuente externa.
- Transformar y enriquecer los datos con estadÃ­sticas relevantes.
- Generar un Ã­ndice de confort basado en temperatura y humedad.
- Cargar el resultado procesado en un archivo JSON para persistencia.
- Ejecutar y monitorear el pipeline de manera completamente automÃ¡tica mediante Airflow.

## ğŸ› ï¸ Stack TecnolÃ³gico

- **Apache Airflow 2.7+** â€“ OrquestaciÃ³n del pipeline
- **Docker & Docker Compose** â€“ Entorno reproducible
- **Python 3.9+** â€“ Transformaciones y lÃ³gica ETL
- **JSON** â€“ Almacenamiento del resultado
- **Airflow PythonOperator** â€“ EjecuciÃ³n de tareas

## ğŸ“ Estructura del Proyecto

weather-pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pipeline_clima.py      # DAG principal ETL
â”œâ”€â”€ docker-compose.yaml         # OrquestaciÃ³n Airflow con Docker
â”œâ”€â”€ requirements.txt            # Dependencias adicionales (opcional)
â””â”€â”€ README.md                   # DocumentaciÃ³n

## ğŸŒ¡ï¸ Â¿QuÃ© problema resuelve este pipeline?

- Este pipeline permite responder preguntas como:
- Â¿CuÃ¡l es la temperatura actual en mi ciudad?
- Â¿CÃ³mo se comporta la humedad junto con la temperatura?
- Â¿QuÃ© tan cÃ³modo se percibe el clima en base a sus condiciones?
- Â¿CÃ³mo registrar automÃ¡ticamente datos del clima para anÃ¡lisis diario/horario?
- Es Ãºtil para dashboards personales, anÃ¡lisis de tendencias o experimentos de datos.

## ğŸ” Flujo del Pipeline (ETL)

**1. Extract**
- Obtiene datos climÃ¡ticos de una API o fuente simulada (ciudad, temperatura, humedad, fecha).

**2. Transform**
- CÃ¡lculo de temperatura en Fahrenheit.
- CÃ¡lculo de un Ãndice de Confort segÃºn reglas simples.
- Enriquecimiento de los datos originales.

**3. Load**
- Almacena los datos transformados en un archivo JSON dentro del contenedor (por defecto /tmp).

## ğŸ–¥ï¸ DAG en Airflow

El pipeline estÃ¡ compuesto por 3 tareas encadenadas:
```bash
extraer â†’ calcular â†’ guardar
```

## ğŸš€ InstalaciÃ³n y EjecuciÃ³n

**1. Prerrequisitos**

- Docker Desktop instalado.
- Docker Compose habilitado.
- 4GB de RAM disponibles.

**2. Clonar el repositorio**
```bash
git clone https://github.com/santiago-izurieta-data/mi-pipeline.git
cd mi-pipeline
```

**3. (Opcional) Agregar librerÃ­as extra**
```bash
requests
pandas
```

**4. Inicializar Airflow**
```bash
docker compose up airflow-init
```

**5. Levantar los servicios**
```bash
docker compose up -d
```

**6. Acceder al UI de Airflow**
- URL: http://localhost:8080
- Usuario: airflow
- Password: airflow

## ğŸ’» Uso
**Activar y ejecutar el DAG**
1. En la interfaz de Airflow, activa el DAG pipeline_clima.
2. Haz clic en â–¶ï¸ para ejecutar manualmente.
3. Observa el flujo desde la vista Graph.
4. Abre los logs para ver detalles de cada tarea.

## ğŸ“Š Resultados

- GeneraciÃ³n automÃ¡tica de archivos JSON diarios/horarios con datos enriquecidos.
- Pipeline confiable y reproducible gracias a Airflow + Docker.
- Flujo ETL modular y extensible.
- Capacidad para conectarse a APIs reales sin modificar la arquitectura.

## ğŸ”„ Pipeline Flow
```bash
API/Sources â†’ EXTRACT â†’ TRANSFORM â†’ LOAD â†’ Archivo JSON (/tmp)
```

## ğŸ“ˆ PrÃ³ximas Mejoras

 - IntegraciÃ³n con una API real como OpenWeatherMap.
 - Guardar datos en PostgreSQL o DuckDB.
 - Dashboard en Grafana o Streamlit.
 - ParametrizaciÃ³n del DAG para mÃºltiples ciudades.
 - Pruebas unitarias (pytest + Airflow DAG tests).

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo licencia MIT. Consulta el archivo LICENSE para mÃ¡s detalles.

## ğŸ‘¤ Autor

Santiago Izurieta

LinkedIn: https://ec.linkedin.com/in/santiago-izurieta-844324125

Portfolio: https://my-data-engineer-folio.lovable.app/

## ğŸ™ Agradecimientos

- Comunidad de Apache Airflow
- Proyecto del taller â€œMi Primer Pipeline con Airflowâ€
- DocumentaciÃ³n oficial de Docker y Airflow